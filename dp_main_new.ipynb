{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPggbURhtse7N4PFl3Qw2Yo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"M25lx_MUMKls"},"outputs":[],"source":["\"\"\"\n","Dynamic Programming, energy management\n","\"\"\"\n","import os\n","import time\n","import numpy as np\n","from tqdm import tqdm\n","import scipy.io as scio\n","from common.arguments import get_args\n","from common.DP_env import DP_Env\n","from common.agentEMS import EMS\n","from common.DP_EMS_agent import DP_EMS_agent\n","\n","class DP_brain:\n","    def __init__(self, env, AgentEMS, DP_EMS_Agent):\n","        self.EMS_Agent = AgentEMS\n","        self.DP_EMS_agent = DP_EMS_Agent\n","        self.env = env\n","        self.gamma = 0.9\n","        self.states = self.env.states\n","        self.actions = self.env.actions\n","        self.policy_as_action_id = np.zeros(self.env.time_steps, dtype=np.float32)\n","        self.policy_as_reward_id = np.zeros(self.env.time_steps, dtype=np.float32)\n","        self.optimal_action_id_table = np.zeros((len(self.states), self.env.time_steps), dtype=np.int32)\n","        self.optimal_reward_table = np.zeros((len(self.states), self.env.time_steps), dtype=np.float32)\n","        self.optimal_reward_id_table = np.zeros(self.optimal_reward_table.shape, dtype=np.int32)\n","        # self.deltas = np.zeros(self.optimal_action_table.shape, dtype=np.float32)\n","        self.info_dict = {'T_mot': [], 'W_mot': [], 'mot_eff': [], 'P_mot': [],\n","                          'P_fc': [], 'P_fce': [], 'fce_eff': [], 'FCS_SOH': [],\n","                          'P_dcdc': [], 'dcdc_eff': [], 'FCS_De': [], 'travel': [],\n","                          'd_s_s': [], 'd_low': [], 'd_high': [], 'd_l_c': [],\n","                          'EMS_reward': [], 'soc_cost': [], 'h2_equal': [], 'h2_fcs': [],\n","                          'money_cost': [], 'h2_money': [], 'batt_money': [], 'fcs_money': [],\n","                          'SOC': [], 'SOH': [], 'I': [], 'I_c': [], 'money_cost_real': [],\n","                          'cell_OCV': [], 'cell_Vt': [], 'cell_V_3': [],\n","                          'cell_power_out': [], 'P_batt': [], 'tep_a': [], 'dsoh': []}\n","\n","    def DP_backward(self):\n","        for time_step in tqdm(range(self.env.time_steps-2, -1, -1)):\n","            # [3438, 3437, ... ,0] [time_steps-1, time_steps-2, ... , 0]\n","            car_spd = self.env.speed_list[time_step]\n","            car_acc = self.env.acc_list[time_step]\n","            # print('\\nstep: %d, car_spd: %.2f, car_acc: %.2f' % (time_step, car_spd, car_acc))\n","            for state_id, state in enumerate(self.states):\n","                state_action_money = np.zeros(len(self.actions), dtype=np.float32)+10000\n","                # print(state_action_money)\n","                state_action_reward = np.zeros(len(self.actions), dtype=np.float32)-10000\n","                for action_id, action in enumerate(self.actions):\n","                    SOC_new = self.DP_EMS_agent.execute(action=action, car_spd=car_spd, car_acc=car_acc, soc=state)\n","                    DP_EMS_reward, money_cost = self.DP_EMS_agent.get_reward(SOC_new)\n","                    state_action_money[action_id] = money_cost   # score every action\n","                    state_action_reward[action_id] = DP_EMS_reward\n","                # choose the action-id with the highest score (minimum cost)\n","                self.optimal_action_id_table[state_id, time_step] = np.argmin(state_action_money)\n","                self.optimal_reward_table[state_id, time_step] = np.max(state_action_reward)\n","                self.optimal_reward_id_table[state_id, time_step] = np.argmax(state_action_reward)\n","            self.optimal_reward_table[:, time_step] +=\\\n","                self.gamma *self.optimal_reward_table[:, time_step+1]\n","\n","    def execute(self, car_spd, car_acc, P_FC):\n","        obs = self.EMS_Agent.execute(action=[P_FC], car_spd=car_spd, car_acc=car_acc)\n","        reward = self.EMS_Agent.get_reward()\n","        info = self.EMS_Agent.get_info()\n","        soc_new = obs[0]\n","        return soc_new, reward, info\n","\n","    def find_s_idx(self, SOC_new):\n","        delta_list = abs(SOC_new-self.env.states)\n","        near_s_id = np.argmin(delta_list)\n","        near_s = self.states[near_s_id]\n","        return near_s_id, near_s\n","\n","    def get_forward_policy(self):\n","        near_s0_list = []\n","        self.EMS_Agent.reset_obs()\n","        s0 = self.env.state_init\n","        s0_idx, near_s0 = self.find_s_idx(s0)\n","        info = {}\n","        for step in range(self.env.time_steps):\n","            near_s0_list.append(near_s0)\n","            car_spd = self.env.speed_list[step]\n","            car_acc = self.env.acc_list[step]\n","            # get action\n","            act_id = self.optimal_action_id_table[s0_idx, step]\n","            act1 = self.actions[act_id]\n","            self.policy_as_action_id[step] = act1\n","            act_id = self.optimal_reward_id_table[s0_idx, step]\n","            act2 = self.actions[act_id]\n","            self.policy_as_reward_id[step] = act2\n","            act = act2\n","\n","            s_new, reward, info = self.execute(car_spd, car_acc, act)\n","            for key in self.info_dict.keys():\n","                self.info_dict[key].append(info[key])\n","\n","            s_idx, near_s = self.find_s_idx(s_new)  # id of new state\n","            near_s0 = near_s\n","            s0_idx = s_idx\n","\n","        self.info_dict.update({'policy_as_action': self.policy_as_action_id.tolist(),\n","                               'policy_as_reward': self.policy_as_reward_id.tolist(),\n","                               'near_s0_list': near_s0_list})\n","        print(\"---dynamic programming finished!---\")\n","        # show data\n","        travel = info['travel']/1000  # km\n","        h2 = sum(self.info_dict['h2_fcs'])  # g\n","        eq_h2 = sum(self.info_dict['h2_equal'])  # g\n","        money = sum(self.info_dict['money_cost'])  # RMB\n","        h2_100 = h2/travel*100\n","        equal_h2_100 = eq_h2/travel*100\n","        m_100 = money/travel*100\n","        self.info_dict.update({'money_100': m_100, 'eq_h2_100': equal_h2_100})\n","        # print\n","        soc = info['SOC']\n","        fcs_soh = info['FCS_SOH']\n","        bat_soh = info['SOH']\n","        print('\\nDP-EMS: travel %.3fkm, SOC %.4f, FCS-SOH %.6f, Bat-SOH %.6f'\n","              %(travel, soc, fcs_soh, bat_soh))\n","        print('DP-EMS: H2_100km %.1fg, eq_H2_100km %.1fg, money_100km ï¿¥%.2f'\n","              %(h2_100, equal_h2_100, m_100))\n","\n","\n","if __name__ == \"__main__\":\n","    strat_tiem = time.time()\n","    args = get_args()\n","    scenario = args.scenario_name  # CTUDC, WVU, JN\n","    dp_env = DP_Env(scenario)\n","    print('scenario name: %s'%scenario)\n","    print('\\nstep %d * state %d * action %d: %d'%\n","          (dp_env.time_steps, dp_env.states.shape[0], dp_env.actions.shape[0],\n","           dp_env.states.shape[0]*dp_env.actions.shape[0]*dp_env.time_steps))\n","    EMSAgent = EMS(w_soc=args.w_soc, soc0=0.5, SOC_MODE=args.MODE,\n","                   abs_spd_MAX=dp_env.abs_spd_MAX, abs_acc_MAX=dp_env.abs_acc_MAX)\n","    DP_EMS_Agent = DP_EMS_agent(w_soc=args.w_soc, gamma=0.9)\n","    DP_brain = DP_brain(dp_env, EMSAgent, DP_EMS_Agent)\n","    DP_brain.DP_backward()\n","    # save data dir\n","    datadir = './DP_result/' + scenario + '_w%d' % args.w_soc + '_'+args.file_v\n","    if not os.path.exists(datadir):\n","        os.makedirs(datadir)\n","    # save backward data\n","    dataname_back1 = '/action_id.mat'\n","    scio.savemat(datadir+dataname_back1, mdict={'action_id': DP_brain.optimal_action_id_table})\n","    dataname_back2 = '/reward_id.mat'\n","    scio.savemat(datadir+dataname_back2, mdict={'reward_id': DP_brain.optimal_reward_id_table})\n","    dataname_back3 = '/optimal_reward.mat'\n","    scio.savemat(datadir+dataname_back3, mdict={'optimal_reward': DP_brain.optimal_reward_table})\n","    end_claculate = time.time()\n","    calculation_time = end_claculate-strat_tiem\n","    print(\"\\ntime for calculation: %.2fs\"%calculation_time)\n","    # forward\n","    DP_brain.get_forward_policy()\n","    dataname = '/DP_EMS_info.mat'\n","    scio.savemat(datadir+dataname, mdict={'DP_EMS_info': DP_brain.info_dict})\n","\n","    end_time = time.time()\n","    spent_time = end_time-end_claculate\n","    print(\"\\ntime for forward_policy: %.2fs\"%spent_time)"]}]}