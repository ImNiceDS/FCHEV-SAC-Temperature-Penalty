{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxQPoE96fx4AQfQxWQ7viB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HFVgcASLDTxb"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as opt\n","from network import QNetwork, GaussianPolicy, BetaPolicy\n","\n","class SAC:\n","    def __init__(self, args):\n","        self.gamma = args.gamma\n","        self.tau = args.tau\n","        self.alpha = args.alpha\n","        self.automatic_entropy_tuning = args.auto_tune\n","        self.device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n","\n","        # soft Q-function\n","        self.critic = QNetwork(args).to(self.device)\n","        self.critic_target = QNetwork(args).to(self.device)\n","        self.critic_target.load_state_dict(self.critic.state_dict())\n","        self.critic_optimizer = opt.Adam(self.critic.parameters())\n","\n","        # policy network\n","        if args.policy == 'Guassian':\n","            print(\"\\n---Guassian policy is employed.---\\n\")\n","            self.actor = GaussianPolicy(args).to(self.device)\n","        else:       # 'Beta'\n","            print(\"\\n---Beta policy is employed.---\\n\")\n","            self.actor = BetaPolicy(args).to(self.device)\n","        self.actor_optimizer = opt.Adam(self.actor.parameters())\n","\n","        # automatic_entropy_tuning\n","        if self.automatic_entropy_tuning is True:\n","            self.target_entropy = float(-args.action_dim)\n","            # self.target_entropy = log(args.action_dim)  # its value is 0, <float>    worse performance\n","            self.log_alpha = torch.ones(1, requires_grad=True, device=self.device)\n","            self.alpha_optimizer = opt.Adam([self.log_alpha], lr=args.lr_alpha)\n","        # device info\n","        print('actor device: ', list(self.actor.parameters())[0].device)\n","        print('critic device: ', list(self.critic.parameters())[0].device)\n","        # print('alpha device: ', list(self.critic.parameters())[0].device)\n","\n","        # learning rate schedulers\n","        step_size = int(args.max_episodes/10)\n","        self.scheduler_lr_cr = opt.lr_scheduler.CyclicLR(self.critic_optimizer, base_lr=args.base_lrs,\n","                                                         max_lr=args.lr_critic, step_size_up=step_size,\n","                                                         mode=\"triangular2\", cycle_momentum=False)\n","        self.scheduler_lr_ac = opt.lr_scheduler.CyclicLR(self.actor_optimizer, base_lr=args.base_lrs,\n","                                                         max_lr=args.lr_actor, step_size_up=step_size,\n","                                                         mode=\"triangular2\", cycle_momentum=False)\n","        self.scheduler_lr_al = opt.lr_scheduler.CyclicLR(self.alpha_optimizer, base_lr=args.base_lrs,\n","                                                         max_lr=args.lr_alpha, step_size_up=step_size,\n","                                                         mode=\"triangular2\", cycle_momentum=False)\n","\n","        if args.load_or_not is False:\n","            # create the directory to store the model\n","            if not os.path.exists(args.save_dir):\n","                os.mkdir(args.save_dir)\n","            self.model_path = args.save_dir+'/'+args.scenario_name+'/'+'net_params'\n","            if not os.path.exists(self.model_path):\n","                os.mkdir(self.model_path)\n","        else:\n","            # load model to evaluate\n","            load_path = args.load_dir+'/'+args.load_scenario_name+'/'+'net_params'\n","            actor_pkl = '/actor_params_ep%d.pkl'%args.load_episode\n","            critic_pkl = '/critic_params_ep%d.pkl'%args.load_episode\n","            load_a = load_path+actor_pkl\n","            load_c = load_path+critic_pkl\n","            if os.path.exists(load_a):\n","                self.actor.load_state_dict(torch.load(load_a))\n","                self.critic.load_state_dict(torch.load(load_c))\n","                print('Agent successfully loaded actor_network: {}'.format(load_a))\n","                print('Agent successfully loaded critic_network: {}'.format(load_c))\n","            else:\n","                print('----Failed to load----')\n","            if self.automatic_entropy_tuning is True:\n","                load_alpha = load_path+'/alpha_params_ep%d.pkl'%args.load_episode\n","                self.alpha_optimizer.load_state_dict(torch.load(load_alpha))\n","                print('Agent successfully loaded alpha_network: {}'.format(load_alpha))\n","\n","    def _soft_update_target_network(self):\n","        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n","            target_param.data.copy_((1-self.tau)*target_param.data+self.tau*param.data)\n","\n","    def select_action(self, state, evaluate=False):\n","        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)  # Tensor(1,1)\n","        if evaluate is False:\n","            action, _, _ = self.actor.get_action(state)\n","        else:\n","            _, _, action = self.actor.get_action(state)\n","        return action.detach().cpu().numpy()[0]\n","\n","    def learn(self, transition):\n","        state_batch = transition[0]\n","        action_batch = transition[1]\n","        reward_batch = transition[2]\n","        next_state_batch = transition[3]\n","        # mask_batch = transition[4]\n","\n","        state_batch = torch.FloatTensor(state_batch).to(self.device)\n","        action_batch = torch.FloatTensor(action_batch).to(self.device)\n","        reward_batch = torch.FloatTensor(reward_batch).to(self.device).unsqueeze(1)\n","        next_state_batch = torch.FloatTensor(next_state_batch).to(self.device)\n","        # mask_batch = torch.FloatTensor(mask_batch).to(self.device).unsqueeze(1)\n","\n","        # Update the Q-function parameters\n","        with torch.no_grad():\n","            next_action, next_log_pi, _ = self.actor.get_action(next_state_batch)\n","            qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_action)\n","            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target)-self.alpha*next_log_pi\n","            # next_q_value = reward_batch + (1-mask_batch)*self.gamma*min_qf_next_target\n","            next_q_value = reward_batch + self.gamma*min_qf_next_target\n","        qf1, qf2 = self.critic(state_batch, action_batch)  # Two Q-functions to mitigate positive bias in the policy improvement step\n","        qf1_loss = F.mse_loss(qf1, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n","        qf2_loss = F.mse_loss(qf2, next_q_value)  # JQ = ùîº(st,at)~D[0.5(Q1(st,at) - r(st,at) - Œ≥(ùîºst+1~p[V(st+1)]))^2]\n","        qf_loss = qf1_loss+qf2_loss\n","\n","        self.critic_optimizer.zero_grad()\n","        qf_loss.backward()\n","        self.critic_optimizer.step()\n","        # soft update target Q-function parameters\n","        self._soft_update_target_network()\n","\n","        # Update policy weights\n","        action, log_pi, _ = self.actor.get_action(state_batch)\n","        qf1_pi, qf2_pi = self.critic(state_batch, action)\n","        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n","        actor_loss = ((self.alpha*log_pi)-min_qf_pi).mean()\n","        # minimizing this: JœÄ = ùîºst‚àºD,Œµt‚àºN[Œ± * logœÄ(f(Œµt;st)|st) ‚àí Q(st,f(Œµt;st))]\n","\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        # Adjust temperature\n","        if self.automatic_entropy_tuning is True:\n","            alpha_loss = -(self.log_alpha*(log_pi+self.target_entropy).detach()).mean()\n","            self.alpha_optimizer.zero_grad()\n","            alpha_loss.backward()\n","            self.alpha_optimizer.step()\n","            self.alpha = self.log_alpha.exp()\n","            alpha_tlogs = self.alpha.clone()  # For TensorboardX logs\n","        else:\n","            alpha_loss = torch.tensor(0.).to(self.device)\n","            alpha_tlogs = torch.tensor(self.alpha)  # For TensorboardX logs\n","        return qf1_loss.item(), qf2_loss.item(), actor_loss.item(), alpha_loss.item(), alpha_tlogs.item()\n","\n","    def save_net(self, episode):\n","        torch.save(self.actor.state_dict(), self.model_path+'/actor_params_ep%d.pkl'%episode)\n","        torch.save(self.critic.state_dict(), self.model_path+'/critic_params_ep%d.pkl'%episode)\n","        if self.automatic_entropy_tuning is True:\n","            torch.save(self.alpha_optimizer.state_dict(), self.model_path+'/alpha_params_ep%d.pkl'%episode)\n","\n","    def lr_scheduler(self):\n","        lrcr = self.scheduler_lr_cr.get_last_lr()[0]\n","        lrac = self.scheduler_lr_ac.get_last_lr()[0]\n","        lral = self.scheduler_lr_al.get_last_lr()[0]\n","        self.scheduler_lr_cr.step()\n","        self.scheduler_lr_ac.step()\n","        self.scheduler_lr_al.step()\n","        return lrcr, lrac, lral"]}]}