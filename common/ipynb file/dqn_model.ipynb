{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/CE5nFvpxrwA+RpWgmwea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_f133acJLC2c"},"outputs":[],"source":["from collections import deque\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","import os\n","\n","class Memory:\n","    def __init__(self, memory_size, batch_size):\n","        memory_size = int(memory_size)\n","        self.memory_size = memory_size\n","        self.batch_size = int(batch_size)\n","        self.counter = 0\n","        self.current_size = 0\n","        self.memory_buffer = deque(maxlen=memory_size)\n","\n","    def store_trasition(self, s, a, r, s_):\n","        transition = (s, a, r, s_)\n","        self.memory_buffer.append(transition)\n","        self.counter += 1\n","        self.current_size = min(self.counter, self.memory_size)\n","\n","    def uniform_sample(self):\n","        temp_buffer = []\n","        idx = np.random.randint(0, self.current_size, self.batch_size)\n","        for i in idx:\n","            temp_buffer.append(self.memory_buffer[i])\n","        return temp_buffer\n","\n","class DQN_net(nn.Module):\n","    def __init__(self, s_dim, a_dim):\n","        super(DQN_net, self).__init__()\n","        self.fc1 = nn.Linear(s_dim, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.fc4 = nn.Linear(64, a_dim)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.relu(self.fc4(x))\n","        return x\n","\n","class DQN_model:\n","    def __init__(self, args, s_dim, a_dim, target_update_freq=500):\n","        self.args = args\n","        self.gamma = 0.9\n","        self.action_dim = a_dim\n","        self.dqn = DQN_net(s_dim=s_dim, a_dim=a_dim)\n","        lr = args.lr_DQN\n","        lr_base = args.base_lrs\n","        self.dqn_optimizer = torch.optim.Adam(self.dqn.parameters())\n","        self.dqn_target = DQN_net(s_dim=s_dim, a_dim=a_dim)\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","        step_size = int(args.max_episodes/10)\n","        self.scheduler_lr = torch.optim.lr_scheduler.CyclicLR(self.dqn_optimizer,\n","                                                                base_lr=lr,\n","                                                                max_lr=lr_base, step_size_up=step_size,\n","                                                                mode=\"triangular2\", cycle_momentum=False)\n","\n","        # reset number of update\n","        self.num_updates = 0\n","        self.target_update_freq = target_update_freq\n","        self.loss = 0.0\n","\n","    def train(self, minibatch):\n","        # obtain minibatch\n","        state = []\n","        action = []\n","        reward = []\n","        state_next = []\n","        for tt in minibatch:\n","            state.append(tt[0])\n","            action.append(tt[1])\n","            reward.append(tt[2])\n","            state_next.append(tt[3])\n","        state = np.array(state)\n","        action = np.array(action)\n","        reward = np.array(reward)\n","        state_next = np.array(state_next)\n","        state = Variable(torch.from_numpy(state)).type(dtype=torch.float32)\n","        action = Variable(torch.from_numpy(action)).type(dtype=torch.int64)\n","        reward = Variable(torch.from_numpy(reward)).type(dtype=torch.float32)\n","        state_next = Variable(torch.from_numpy(state_next)).type(dtype=torch.float32)\n","\n","        Q_value = self.dqn(state)\n","        action = torch.unsqueeze(action, dim=1)\n","        Q_value = Q_value.gather(1, action)  # Q(s, a)\n","        Q_next = self.dqn_target(state_next).detach()\n","        Q_next_max, idx = Q_next.max(1)  # greedy policy\n","        Q_target = reward+self.gamma*Q_next_max\n","        Q_target = torch.unsqueeze(Q_target, dim=1)\n","        loss_fn = nn.MSELoss(reduction='mean')\n","        loss = loss_fn(Q_value, Q_target)\n","        self.loss = loss.data\n","\n","        self.dqn_optimizer.zero_grad()\n","        loss.backward()\n","        self.dqn_optimizer.step()\n","        self.num_updates += 1\n","        if self.num_updates%self.target_update_freq == 0:\n","            self.dqn_target.load_state_dict(self.dqn.state_dict())\n","\n","    def e_greedy_action(self, s, epsilon):\n","        s = Variable(torch.from_numpy(s)).type(dtype=torch.float32)\n","        Q_value = self.dqn.forward(s)\n","        if np.random.random() < epsilon:\n","            return np.random.randint(0, self.action_dim), epsilon\n","        else:\n","            return np.argmax(Q_value), epsilon\n","\n","    def save_model(self, save_path, save_episode):\n","        model_path = save_path+'/net_params'\n","        if not os.path.exists(model_path):\n","            os.makedirs(model_path)\n","        torch.save(self.dqn.state_dict(), model_path +\n","                   '/dqn_params_ep%d.pkl'%save_episode)"]}]}