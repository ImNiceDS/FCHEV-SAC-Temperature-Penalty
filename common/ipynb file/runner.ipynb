{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPz32Hhh2y/1XiHs7lfvhwZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4azn8i1JE6LR"},"outputs":[],"source":["import datetime\n","import os\n","import numpy as np\n","from numpy.random import normal  # normal distribution\n","import scipy.io as scio\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","from tqdm import tqdm\n","from memory import MemoryBuffer\n","from sac import SAC\n","from ddpg import DDPG\n","\n","class Runner:\n","    def __init__(self, args, env):\n","        self.args = args\n","        self.env = env\n","        self.buffer = MemoryBuffer(args)\n","        self.SAC_agent = SAC(args)\n","        self.DDPG_agent = DDPG(args)\n","        # configuration\n","        self.episode_num = args.max_episodes\n","        self.episode_step = args.episode_steps\n","        self.DONE = {}\n","        self.save_path = self.args.save_dir+'/'+self.args.scenario_name\n","        if not os.path.exists(self.save_path):\n","            os.makedirs(self.save_path)\n","        self.save_path_episode = self.save_path+'/episode_data'\n","        if not os.path.exists(self.save_path_episode):\n","            os.makedirs(self.save_path_episode)\n","        # random seed\n","        if self.args.random_seed:\n","            self.seed = np.random.randint(100)\n","        else:\n","            self.seed = 93\n","        # tensorboard\n","        fileinfo = args.scenario_name\n","        self.writer = SummaryWriter(args.log_dir+'/{}_{}_{}_seed{}'.format\n","                                    (datetime.datetime.now().strftime(\"%m-%d_%H-%M\"),\n","                                     fileinfo, self.args.DRL, self.seed))\n","\n","    def set_seed(self):\n","        np.random.seed(self.seed)\n","        torch.manual_seed(self.seed)\n","        torch.cuda.manual_seed_all(self.seed)\n","        print(\"Random seeds have been set to %d !\\n\"%self.seed)\n","\n","    def run_SAC(self):\n","        average_reward = []  # average_reward of each episode\n","        c_loss_1 = []\n","        c_loss_2 = []\n","        a_loss = []\n","        en_loss = []\n","        h2_100_list = []\n","        eq_h2_100_list = []  # equivalent hydrogen consumption per 100 km\n","        money_100_list = []  # money spent per 100 km\n","        FCS_SoH = []\n","        Batt_SoH = []\n","        SOC = []\n","        lr_recorder = {'lrcr': [], 'lrac': [], 'lral': []}\n","        updates = 0  # for tensorboard counter\n","        for episode in tqdm(range(self.episode_num)):\n","            state = self.env.reset()  # reset the environment\n","            episode_reward = []\n","            c_loss_1_one_ep = []\n","            c_loss_2_one_ep = []\n","            a_loss_one_ep = []\n","            en_loss_one_ep = []\n","            alpha_value_ep = []\n","            info = []\n","            # data being saved in .mat\n","            episode_info = {'T_mot': [], 'W_mot': [], 'mot_eff': [], 'P_mot': [],\n","                            'P_fc': [], 'P_fce': [], 'fce_eff': [], 'FCS_SOH': [],\n","                            'P_dcdc': [], 'dcdc_eff': [], 'FCS_De': [], 'travel': [],\n","                            'd_s_s': [], 'd_low': [], 'd_high': [], 'd_l_c': [],\n","                            'EMS_reward': [], 'soc_cost': [], 'h2_equal': [], 'h2_fcs': [],\n","                            'money_cost': [], 'h2_money': [], 'batt_money': [], 'fcs_money': [],\n","                            'SOC': [], 'SOH': [], 'I': [], 'I_c': [], 'money_cost_real': [],\n","                            'cell_OCV': [], 'cell_Vt': [], 'cell_V_3': [],\n","                            'cell_power_out': [], 'P_batt': [], 'tep_a': [], 'dsoh': []}\n","\n","            for episode_step in range(self.episode_step):\n","                with torch.no_grad():\n","                    action = self.SAC_agent.select_action(state)                          # ★★★★★★★★★★★★★\n","                state_next, reward, done, info = self.env.step(action, episode_step)                                  # ★★★★★★★★★★★★★\n","                # when done is True: unsafe, stop\n","                self.buffer.store(state, action, reward, state_next, done)\n","                if done and episode not in self.DONE.keys():\n","                    print('\\nfailure in step %d of episode %d'%(episode_step, episode))\n","                    self.DONE.update({episode: episode_step})\n","                    # break\n","                state = state_next\n","                # save data\n","                for key in episode_info.keys():\n","                    episode_info[key].append(info[key])\n","                episode_reward.append(reward)\n","                # learn\n","                if self.buffer.currentSize >= 10*self.args.batch_size:\n","                    transition = self.buffer.random_sample()\n","                    critic_loss_1, critic_loss_2, actor_loss, alpha_loss, alpha = self.SAC_agent.learn(transition)\n","                    # save to tensorboard\n","                    self.writer.add_scalar('loss/critic_1', critic_loss_1, updates)\n","                    self.writer.add_scalar('loss/critic_2', critic_loss_2, updates)\n","                    self.writer.add_scalar('loss/actor', actor_loss, updates)\n","                    self.writer.add_scalar('loss/alpha_loss', alpha_loss, updates)\n","                    self.writer.add_scalar('entropy/alpha_value', alpha, updates)\n","                    self.writer.add_scalar('reward/step_reward', reward, updates)\n","                    updates += 1\n","                    # save in .mat\n","                    c_loss_1_one_ep.append(critic_loss_1)\n","                    c_loss_2_one_ep.append(critic_loss_2)\n","                    a_loss_one_ep.append(actor_loss)\n","                    en_loss_one_ep.append(alpha_loss)\n","                    alpha_value_ep.append(alpha)\n","                # save data in .mat\n","                # if episode_step+1 == self.episode_step:\n","                if self.episode_num-episode<=400 and episode_step+1 == self.episode_step:\n","                    # only save the last 10 episode\n","                    # save alpha value of each time step in each episode\n","                    episode_info.update({'alpha': alpha_value_ep})\n","                    # save network parameters\n","                    self.SAC_agent.save_net(episode)\n","                    # save all data in one episode info\n","                    datadir = self.save_path_episode+'/data_ep%d.mat'%episode\n","                    scio.savemat(datadir, mdict=episode_info)\n","            # lr sheduler\n","            lrcr, lrac, lral = self.SAC_agent.lr_scheduler()\n","            lr_recorder['lrcr'].append(lrcr)\n","            lr_recorder['lrac'].append(lrac)\n","            lr_recorder['lral'].append(lral)\n","            # show episode data\n","            travel = info['travel']/1000  # km\n","            h2 = sum(episode_info['h2_fcs'])  # g\n","            eq_h2 = sum(episode_info['h2_equal'])  # g\n","            money = sum(episode_info['money_cost_real'])  # RMB\n","            h2_100 = h2/travel*100\n","            equal_h2_100 = eq_h2/travel*100\n","            m_100 = money/travel*100\n","            h2_100_list.append(h2_100)\n","            eq_h2_100_list.append(equal_h2_100)\n","            money_100_list.append(m_100)\n","            # print\n","            soc = info['SOC']\n","            fcs_soh = info['FCS_SOH']\n","            bat_soh = info['SOH']\n","            print('\\nepi %d: travel %.3fkm, SOC %.4f, FCS-SOH %.6f, Bat-SOH %.6f'\n","                  % (episode, travel, soc, fcs_soh, bat_soh))\n","            print('epi %d: H2_100km %.1fg, eq_H2_100km %.1fg, money_100km ￥%.2f'%\n","                  (episode, h2_100, equal_h2_100, m_100))\n","            # save loss and reward on the average\n","            ep_r = np.mean(episode_reward)\n","            ep_c1 = np.mean(c_loss_1_one_ep)\n","            ep_c2 = np.mean(c_loss_2_one_ep)\n","            ep_a = np.mean(a_loss_one_ep)\n","            ep_en = np.mean(en_loss_one_ep)\n","            print('epi %d: ep_r %.3f, c-loss1 %.4f, c-loss2 %.4f, a-loss %.4f, en-loss %.4f'\n","                  % (episode, ep_r, ep_c1, ep_c2, ep_a, ep_en))\n","            print('epi %d: lr_critic %.6f, lr_actor %.6f, lr_alpha %.6f' % (episode, lrcr, lrac, lral))\n","            average_reward.append(ep_r)\n","            c_loss_1.append(ep_c1)\n","            c_loss_2.append(ep_c2)\n","            a_loss.append(ep_a)\n","            en_loss.append(ep_en)\n","            FCS_SoH.append(fcs_soh)\n","            Batt_SoH.append(bat_soh)\n","            SOC.append(soc)\n","\n","        scio.savemat(self.save_path+'/reward.mat', mdict={'reward': average_reward})\n","        scio.savemat(self.save_path+'/critic_loss.mat', mdict={'c_loss_1': c_loss_1, 'c_loss_2': c_loss_2})\n","        scio.savemat(self.save_path+'/actor_loss.mat', mdict={'a_loss': a_loss})\n","        scio.savemat(self.save_path+'/entropy_loss.mat', mdict={'en_loss': en_loss})\n","        scio.savemat(self.save_path+'/lr_recorder.mat', mdict=lr_recorder)\n","        scio.savemat(self.save_path+'/h2.mat', mdict={'h2': h2_100_list})\n","        scio.savemat(self.save_path+'/eq_h2.mat', mdict={'eq_h2': eq_h2_100_list})\n","        scio.savemat(self.save_path+'/money.mat', mdict={'money': money_100_list})\n","        scio.savemat(self.save_path+'/FCS_SOH.mat', mdict={'FCS_SOH': FCS_SoH})\n","        scio.savemat(self.save_path+'/Batt_SOH.mat', mdict={'Batt_SOH': Batt_SoH})\n","        scio.savemat(self.save_path+'/SOC.mat', mdict={'SOC': SOC})\n","\n","    def run_DDPG(self):\n","        noise_decrease = False\n","        noise_rate = self.args.noise_rate\n","        average_reward = []  # average_reward of each episode\n","        c_loss = []\n","        a_loss = []\n","        h2_100_list = []\n","        eq_h2_100_list = []  # equivalent hydrogen consumption per 100 km\n","        money_100_list = []  # money spent per 100 km\n","        FCS_SoH = []\n","        Batt_SoH = []\n","        SOC = []\n","        lr_recorder = {'lrcr': [], 'lrac': []}\n","        noise = []\n","        updates = 0  # for tensorboard counter\n","        for episode in tqdm(range(self.episode_num)):\n","            state = self.env.reset()  # reset the environment\n","            if noise_decrease:\n","                noise_rate *= self.args.noise_discount_rate\n","            episode_reward = []\n","            c_loss_one_ep = []\n","            a_loss_one_ep = []\n","            info = []\n","            # data being saved in .mat\n","            episode_info = {'T_mot': [], 'W_mot': [], 'mot_eff': [], 'P_mot': [],\n","                            'P_fc': [], 'P_fce': [], 'fce_eff': [], 'FCS_SOH': [],\n","                            'P_dcdc': [], 'dcdc_eff': [], 'FCS_De': [], 'travel': [],\n","                            'd_s_s': [], 'd_low': [], 'd_high': [], 'd_l_c': [],\n","                            'EMS_reward': [], 'soc_cost': [], 'h2_equal': [], 'h2_fcs': [],\n","                            'money_cost': [], 'h2_money': [], 'batt_money': [], 'fcs_money': [],\n","                            'SOC': [], 'SOH': [], 'I': [], 'I_c': [], 'money_cost_real': [],\n","                            'cell_OCV': [], 'cell_Vt': [], 'cell_V_3': [],\n","                            'cell_power_out': [], 'P_batt': [], 'tep_a': [], 'dsoh': []}\n","\n","            for episode_step in range(self.episode_step):\n","                with torch.no_grad():\n","                    raw_action = self.DDPG_agent.select_action(state)\n","                action = np.clip(normal(raw_action, noise_rate), -1, 1)\n","                state_next, reward, done, info = self.env.step(action, episode_step)\n","                self.buffer.store(state, action, reward, state_next, done)\n","                if done and episode not in self.DONE.keys():\n","                    print('failure in step %d of episode %d'%(episode_step, episode))\n","                    self.DONE.update({episode: episode_step})\n","                    # break\n","                state = state_next\n","                # save data\n","                for key in episode_info.keys():\n","                    episode_info[key].append(info[key])\n","                episode_reward.append(reward)\n","                # learn\n","                if self.buffer.currentSize >= 10*self.args.batch_size:\n","                    noise_decrease = True\n","                    transition = self.buffer.random_sample()\n","                    critic_loss, actor_loss = self.DDPG_agent.train(transition)\n","                    # save to tensorboard\n","                    self.writer.add_scalar('loss/critic', critic_loss, updates)\n","                    self.writer.add_scalar('loss/actor', actor_loss, updates)\n","                    self.writer.add_scalar('reward/step_reward', reward, updates)\n","                    updates += 1\n","                    # save in .mat\n","                    c_loss_one_ep.append(critic_loss)\n","                    a_loss_one_ep.append(actor_loss)\n","                # save data in .mat     # only save the last 10 episode\n","                if self.episode_num-episode<=50 and episode_step+1 == self.episode_step:\n","                    self.DDPG_agent.save_net(episode)\n","                    datadir = self.save_path_episode+'/data_ep%d.mat'%episode\n","                    scio.savemat(datadir, mdict=episode_info)\n","            # lr sheduler\n","            lra, lrc = self.DDPG_agent.lr_scheduler()\n","            lr_recorder['lrcr'].append(lrc)\n","            lr_recorder['lrac'].append(lra)\n","            # show episode data\n","            travel = info['travel']/1000  # km\n","            h2 = sum(episode_info['h2_fcs'])  # g\n","            eq_h2 = sum(episode_info['h2_equal'])  # g\n","            money = sum(episode_info['money_cost_real'])  # RMB\n","            h2_100 = h2/travel*100\n","            equal_h2_100 = eq_h2/travel*100\n","            m_100 = money/travel*100\n","            h2_100_list.append(h2_100)\n","            eq_h2_100_list.append(equal_h2_100)\n","            money_100_list.append(m_100)\n","            # print\n","            soc = info['SOC']\n","            fcs_soh = info['FCS_SOH']\n","            bat_soh = info['SOH']\n","            print('\\nepi %d: travel %.3fkm, SOC %.4f, FCS-SOH %.6f, Bat-SOH %.6f'\n","                  % (episode, travel, soc, fcs_soh, bat_soh))\n","            print('epi %d: H2_100km %.1fg, eq_H2_100km %.1fg, money_100km ￥%.2f'%\n","                  (episode, h2_100, equal_h2_100, m_100))\n","            # save loss and reward on the average\n","            ep_r = np.mean(episode_reward)\n","            ep_c = np.mean(c_loss_one_ep)\n","            ep_a = np.mean(a_loss_one_ep)\n","            print('epi %d: ep_r %.3f, c-loss %.4f, a-loss %.4f'\n","                  % (episode, ep_r, ep_c, ep_a))\n","            print('epi %d: noise_rate %.6f, lr_critic %.6f, lr_actor %.6f'\n","                  % (episode, noise_rate, lra, lrc))\n","            average_reward.append(ep_r)\n","            c_loss.append(ep_c)\n","            a_loss.append(ep_a)\n","            FCS_SoH.append(fcs_soh)\n","            Batt_SoH.append(bat_soh)\n","            SOC.append(soc)\n","            noise.append(noise_rate)\n","\n","        scio.savemat(self.save_path+'/reward.mat', mdict={'reward': average_reward})\n","        scio.savemat(self.save_path+'/critic_loss.mat', mdict={'c_loss': c_loss})\n","        scio.savemat(self.save_path+'/actor_loss.mat', mdict={'a_loss': a_loss})\n","        scio.savemat(self.save_path+'/lr_recorder.mat', mdict=lr_recorder)\n","        scio.savemat(self.save_path+'/h2.mat', mdict={'h2': h2_100_list})\n","        scio.savemat(self.save_path+'/eq_h2.mat', mdict={'eq_h2': eq_h2_100_list})\n","        scio.savemat(self.save_path+'/money.mat', mdict={'money': money_100_list})\n","        scio.savemat(self.save_path+'/FCS_SoH.mat', mdict={'FCS_SoH': FCS_SoH})\n","        scio.savemat(self.save_path+'/Batt_SoH.mat', mdict={'Batt_SoH': Batt_SoH})\n","        scio.savemat(self.save_path+'/SOC.mat', mdict={'SOC': SOC})\n","        scio.savemat(self.save_path+'/noise.mat', mdict={'noise': noise})\n","\n","    def memory_info(self):\n","        print('\\nbuffer counter:', self.buffer.counter)\n","        print('buffer current size:', self.buffer.currentSize)\n","        print('replay ratio: %.3f'%(self.buffer.counter/self.buffer.currentSize))\n","        print('failure:', self.DONE)"]}]}