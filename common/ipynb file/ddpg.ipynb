{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjXyl6Yt72rVoE/41O31q8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jtGoek8fKTB6"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.optim as opt\n","import os\n","\n","\n","class DDPG:\n","    def __init__(self, args):\n","        self.args = args\n","        self.gamma = args.gamma\n","        load_or_not = any([args.load_or_not, args.evaluate])\n","        # self.c_loss = 0\n","        # self.a_loss = 0\n","\n","        # create the network\n","        self.actor_network = Actor(args)\n","        self.critic_network = Critic(args)\n","\n","        # build up the target network\n","        self.actor_target_network = Actor(args)\n","        self.critic_target_network = Critic(args)\n","\n","        # load the weights into the target networks\n","        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n","        self.critic_target_network.load_state_dict(self.critic_network.state_dict())\n","\n","        # create the optimizer\n","        self.actor_optimizer = opt.Adam(self.actor_network.parameters())\n","        self.critic_optimizer = opt.Adam(self.critic_network.parameters())\n","\n","        # learning rate scheduler\n","        base_lr_a = args.base_lrs\n","        base_lr_c = args.base_lrs\n","        lr_c = args.lr_critic\n","        lr_a = args.lr_actor\n","        step_size = int(args.max_episodes/10)\n","        self.scheduler_lr_a = opt.lr_scheduler.CyclicLR(self.actor_optimizer,\n","                                                        base_lr=base_lr_a, max_lr=lr_a, step_size_up=step_size,\n","                                                        mode=\"triangular2\", cycle_momentum=False)\n","        self.scheduler_lr_c = opt.lr_scheduler.CyclicLR(self.critic_optimizer,\n","                                                        base_lr=base_lr_c, max_lr=lr_c, step_size_up=step_size,\n","                                                        mode=\"triangular2\", cycle_momentum=False)\n","\n","        # create the direction for store the model\n","        if load_or_not is False:\n","            if not os.path.exists(self.args.save_dir):\n","                os.mkdir(self.args.save_dir)\n","            # path to save the model\n","            self.model_path = self.args.save_dir+'/'+self.args.scenario_name\n","            if not os.path.exists(self.model_path):\n","                os.mkdir(self.model_path)\n","        else:\n","            # load model\n","            load_path = self.args.load_dir+'/'+self.args.load_scenario_name+'/net_params'\n","            actor_pkl = '/actor_params_ep%d.pkl'%self.args.load_episode\n","            critic_pkl = '/critic_params_ep%d.pkl'%self.args.load_episode\n","            load_a = load_path+actor_pkl\n","            load_c = load_path+critic_pkl\n","            if os.path.exists(load_a):\n","                self.actor_network.load_state_dict(torch.load(load_a))\n","                self.critic_network.load_state_dict(torch.load(load_c))\n","                print('Agent successfully loaded actor_network: {}'.format(load_a))\n","                print('Agent successfully loaded critic_network: {}'.format(load_c))\n","\n","    # soft update for a single agent\n","    def _soft_update_target_network(self):\n","        for target_param, param in zip(self.actor_target_network.parameters(), self.actor_network.parameters()):\n","            target_param.data.copy_((1-self.args.tau)*target_param.data+self.args.tau*param.data)\n","\n","        for target_param, param in zip(self.critic_target_network.parameters(), self.critic_network.parameters()):\n","            target_param.data.copy_((1-self.args.tau)*target_param.data+self.args.tau*param.data)\n","\n","    # choose action\n","    def select_action(self, state):\n","        state = Variable(torch.from_numpy(state))\n","        # action = self.actor_network.forward(state).detach()\n","        action = self.actor_network.forward(state)\n","        new_action = action.data.numpy()\n","        return new_action\n","\n","    # update the network\n","    def train(self, transition):\n","        state_batch = transition[0]\n","        action_batch = transition[1]\n","        reward_batch = transition[2]\n","        next_state_batch = transition[3]\n","\n","        state = Variable(torch.from_numpy(state_batch))\n","        action = Variable(torch.from_numpy(action_batch))\n","        reward = Variable(torch.from_numpy(reward_batch))\n","        s_next = Variable(torch.from_numpy(next_state_batch))\n","\n","        # ---------------------- optimize critic ----------------------\n","        # Use target actor exploitation policy here for loss evaluation\n","        a_next = self.actor_target_network.forward(s_next).detach()\n","        next_Q = torch.squeeze(self.critic_target_network.forward(s_next, a_next).detach())\n","        y_expected = reward+self.gamma*next_Q\n","        y_predicted = torch.squeeze(self.critic_network.forward(state, action))\n","        # compute critic loss, and update the critic\n","        loss_critic = F.mse_loss(y_predicted, y_expected)\n","        self.critic_optimizer.zero_grad()\n","        loss_critic.backward()\n","        self.critic_optimizer.step()\n","        # self.c_loss = loss_critic.data\n","        c_loss = loss_critic.data\n","\n","        # ---------------------- optimize actor ----------------------\n","        pred_a = self.actor_network.forward(state)\n","        loss_actor = -torch.mean(self.critic_network.forward(state, pred_a))\n","        self.actor_optimizer.zero_grad()\n","        loss_actor.backward()\n","        self.actor_optimizer.step()\n","        # self.a_loss = -loss_actor.data\n","        a_loss = -loss_actor.data\n","\n","        # soft update\n","        self._soft_update_target_network()\n","\n","        return c_loss, a_loss\n","\n","    def save_net(self, save_episode):\n","        model_path = os.path.join(self.args.save_dir, self.args.scenario_name)\n","        if not os.path.exists(model_path):\n","            os.makedirs(model_path)\n","        model_path = os.path.join(model_path, 'net_params')\n","        if not os.path.exists(model_path):\n","            os.makedirs(model_path)\n","        torch.save(self.actor_network.state_dict(), model_path +\n","                   '/actor_params_ep%d.pkl'%save_episode)\n","        torch.save(self.critic_network.state_dict(), model_path +\n","                   '/critic_params_ep%d.pkl'%save_episode)\n","\n","    def lr_scheduler(self):\n","        lra = self.scheduler_lr_a.get_last_lr()\n","        lrc = self.scheduler_lr_c.get_last_lr()\n","        self.scheduler_lr_a.step()\n","        self.scheduler_lr_c.step()\n","        return lra[0], lrc[0]\n","\n","# define the actor network\n","class Actor(nn.Module):\n","    def __init__(self, args):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(args.obs_dim, 64)\n","        self.fc2 = nn.Linear(64, 128)\n","        self.fc3 = nn.Linear(128, 64)\n","        self.action_out = nn.Linear(64, args.action_dim)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        action = torch.tanh(self.action_out(x))  # tanh value section: [-1, 1]\n","        return action\n","\n","class Critic(nn.Module):\n","    def __init__(self, args):\n","        super(Critic, self).__init__()\n","        self.fc1 = nn.Linear(args.obs_dim+args.action_dim, 256)\n","        self.fc2 = nn.Linear(256, 128)\n","        self.fc3 = nn.Linear(128, 128)\n","        self.q_out = nn.Linear(128, 1)\n","\n","    def forward(self, state, action):\n","        x = torch.cat([state, action], dim=1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        q_value = self.q_out(x)\n","        return q_value"]}]}