{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMAGnOycJd1nQT2Juuul3bf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZDskQ7Gi1r2P"},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","import torch\n","import os\n","import scipy.io as scio\n","from common.dqn_model import DQN_model, Memory\n","from common.arguments import get_args\n","from common.agentEMS import EMS\n","from common.utils import get_driving_cycle, get_acc_limit\n","\n","def main_EMS(args, speed_list, acc_list, episodes=400):\n","    save_path = args.save_dir+\"_DQN_\"+args.MODE+'/' \\\n","                +args.scenario_name+\"_w%d\"%args.w_soc+'_' \\\n","                +\"LR%.0e\"%args.lr_DQN+ '_' +args.file_v\n","    if not os.path.exists(save_path):\n","        os.makedirs(save_path)\n","    save_path_episode = save_path+'/episode_data'\n","    if not os.path.exists(save_path_episode):\n","        os.makedirs(save_path_episode)\n","\n","    abs_spd_MAX = max(abs(speed_list))\n","    abs_acc_MAX = max(abs(max(acc_list)), abs(min(acc_list)))\n","    action_num = 60\n","    action_space = np.linspace(0, 1, action_num, dtype=np.float32)\n","    ems = EMS(args.w_soc, args.soc0, args.MODE, abs_spd_MAX, abs_acc_MAX)\n","    memory = Memory(memory_size=args.buffer_size, batch_size=args.batch_size)\n","    dqn_agent = DQN_model(args, s_dim=ems.obs_num, a_dim=action_num)\n","\n","    average_reward = []  # average_reward of each episode\n","    average_loss = []\n","    DONE = {}\n","    lr = []\n","    epsilon_list = []\n","    h2_100_list = []\n","    eq_h2_100_list = []  # equivalent hydrogen consumption per 100 km\n","    money_100_list = []  # money spent per 100 km\n","    FCS_SoH = []\n","    Batt_SoH = []\n","    SOC = []\n","\n","    initial_epsilon = 1.0\n","    finial_epsilon = 0.2\n","    epsilon_decent = (initial_epsilon-finial_epsilon)/150\n","    epsilon = initial_epsilon\n","\n","    SPD_LIST = speed_list\n","    ACC_LIST = acc_list\n","    episode_step_num = SPD_LIST.shape[0]\n","    MILE = np.sum(SPD_LIST)/1000\n","    print('mileage: %.3fkm'%MILE)\n","\n","    for episode in tqdm(range(episodes)):\n","        state = ems.reset_obs()  # ndarray\n","        rewards = []\n","        loss = []\n","        info = []\n","        episode_info = {'T_mot': [], 'W_mot': [], 'mot_eff': [], 'P_mot': [],\n","                        'P_fc': [], 'P_fce': [], 'fce_eff': [], 'FCS_SOH': [],\n","                        'P_dcdc': [], 'dcdc_eff': [], 'FCS_De': [], 'travel': [],\n","                        'd_s_s': [], 'd_low': [], 'd_high': [], 'd_l_c': [],\n","                        'EMS_reward': [], 'soc_cost': [], 'h2_equal': [], 'h2_fcs': [],\n","                        'money_cost': [], 'h2_money': [], 'batt_money': [], 'fcs_money': [],\n","                        'SOC': [], 'SOH': [], 'I': [], 'I_c': [], 'money_cost_real': [],\n","                        'cell_OCV': [], 'cell_Vt': [], 'cell_V_3': [],\n","                        'cell_power_out': [], 'P_batt': [], 'tep_a': [], 'dsoh': []}\n","        for episode_step in range(episode_step_num):\n","            with torch.no_grad():\n","                action_id, epsilon_using = dqn_agent.e_greedy_action(state, epsilon)\n","            action = [action_space[action_id]]  # [float]\n","            spd = SPD_LIST[episode_step]\n","            acc = ACC_LIST[episode_step]\n","            next_state = ems.execute(action, spd, acc)\n","            reward = ems.get_reward()\n","            done = ems.get_done()\n","            info = ems.get_info()\n","\n","            memory.store_trasition(state, action_id, reward, next_state)\n","            state = next_state\n","\n","            rewards.append(reward)\n","            if done and episode not in DONE.keys():\n","                print('\\nSOC failure in step %d of episode %d'%(episode_step, episode))\n","                DONE.update({episode: episode_step})\n","                # break\n","            for key in episode_info.keys():\n","                episode_info[key].append(info[key])\n","\n","            if memory.current_size > 100*args.batch_size:\n","                minibatch = memory.uniform_sample()\n","                dqn_agent.train(minibatch)\n","                loss.append(dqn_agent.loss)\n","\n","            # end of an episode: sava model params,\n","            if episodes-episode <= 20 and episode_step+1 == episode_step_num:\n","                dqn_agent.save_model(save_path, episode)\n","                datadir = save_path_episode+'/data_ep%d.mat'%episode\n","                scio.savemat(datadir, mdict=episode_info)\n","        # end of one episode: save data, print info\n","        # show episode data\n","        travel = info['travel']/1000  # km\n","        h2 = sum(episode_info['h2_fcs'])  # g\n","        eq_h2 = sum(episode_info['h2_equal'])  # g\n","        money = sum(episode_info['money_cost_real'])  # RMB\n","        h2_100 = h2/travel*100\n","        equal_h2_100 = eq_h2/travel*100\n","        m_100 = money/travel*100\n","        h2_100_list.append(h2_100)\n","        eq_h2_100_list.append(equal_h2_100)\n","        money_100_list.append(m_100)\n","        # print\n","        soc = info['SOC']\n","        fcs_soh = info['FCS_SOH']\n","        bat_soh = info['SOH']\n","        FCS_SoH.append(fcs_soh)\n","        Batt_SoH.append(bat_soh)\n","        SOC.append(soc)\n","        print('\\nepi %d: travel %.3fkm, SOC %.4f, FCS-SOH %.6f, Bat-SOH %.6f'\n","              %(episode, travel, soc, fcs_soh, bat_soh))\n","        print('epi %d: H2_100km %.1fg, eq_H2_100km %.1fg, money_100km ï¿¥%.2f'%\n","              (episode, h2_100, equal_h2_100, m_100))\n","        # save loss and reward on average\n","        ep_r = np.mean(rewards)\n","        ep_loss = np.mean(loss)\n","        average_reward.append(ep_r)\n","        average_loss.append(ep_loss)\n","        lr0 = dqn_agent.scheduler_lr.get_last_lr()[0]\n","        lr.append(lr0)\n","        epsilon_list.append(epsilon_using)\n","        print('epi %d: reward %.6f, loss %.6f, epsilon %.6f, lr %.6f'\n","              %(episode, ep_r, ep_loss, epsilon_using, lr0))\n","        epsilon -= float(epsilon_decent)\n","        epsilon = max(epsilon, finial_epsilon)\n","        dqn_agent.scheduler_lr.step()\n","\n","    scio.savemat(save_path+'/lr.mat', mdict={'lr': lr})\n","    scio.savemat(save_path+'/epsilon_list.mat', mdict={'epsilon': epsilon_list})\n","    scio.savemat(save_path+'/loss.mat', mdict={'loss': average_loss})\n","    scio.savemat(save_path+'/reward.mat', mdict={'reward': average_reward})\n","    scio.savemat(save_path+'/h2.mat', mdict={'h2': h2_100_list})\n","    scio.savemat(save_path+'/eq_h2.mat', mdict={'eq_h2': eq_h2_100_list})\n","    scio.savemat(save_path+'/money.mat', mdict={'money': money_100_list})\n","    scio.savemat(save_path+'/FCS_SOH.mat', mdict={'FCS_SOH': FCS_SoH})\n","    scio.savemat(save_path+'/Batt_SOH.mat', mdict={'Batt_SOH': Batt_SoH})\n","    scio.savemat(save_path+'/SOC.mat', mdict={'SOC': SOC})\n","\n","    print('buffer counter:', memory.counter)\n","    print('buffer current size:', memory.current_size)\n","    print('replay ratio: %.3f'%(memory.counter/memory.current_size)+'\\n')\n","    print('done:', DONE)\n","\n","if __name__ == '__main__':\n","    args = get_args()\n","    speed_list = get_driving_cycle(cycle_name=args.scenario_name)\n","    acc_list = get_acc_limit(speed_list, output_max_min=False)\n","\n","    seed = np.random.randint(100)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    print(\"Random seeds have been set to %d!\"%seed)\n","    print('cycle name: ', args.scenario_name)\n","\n","    main_EMS(args, speed_list, acc_list)"]}]}